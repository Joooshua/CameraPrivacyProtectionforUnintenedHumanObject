{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import seaborn as sns\n",
    "# PyTorch\n",
    "from torchvision import transforms, models\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Data science tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# Useful for examining network\n",
    "from torchsummary import summary\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on gpu: False\n"
     ]
    }
   ],
   "source": [
    "# Location of data\n",
    "datadir = './Dataset/MTCNNAlignedCroppedFaces/'\n",
    "traindir = datadir + 'train/'\n",
    "validdir = datadir + 'val/'\n",
    "testdir = datadir + 'test/'\n",
    "save_file_name = 'face_selection-1039.pt'\n",
    "checkpoint_path = 'face_selection-1039.pth'\n",
    "\n",
    "# Change to fit hardware\n",
    "batch_size = 8\n",
    "\n",
    "# Whether to train on a gpu\n",
    "train_on_gpu = cuda.is_available()\n",
    "#train_on_gpu = False\n",
    "print(f'Train on gpu: {train_on_gpu}')\n",
    "\n",
    "# Number of gpus\n",
    "if train_on_gpu:\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f'{gpu_count} gpus detected.')\n",
    "    if gpu_count > 1:\n",
    "        multi_gpu = True\n",
    "    else:\n",
    "        multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x : (os.path.join(datadir, x))\n",
    "                    for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00000ae5e7fcc87222f1fb6f221e7501558e5b08_0', '00000ae5e7fcc87222f1fb6f221e7501558e5b08_1', '00000ae5e7fcc87222f1fb6f221e7501558e5b08_2', '00000ae5e7fcc87222f1fb6f221e7501558e5b08_3', '00000ae5e7fcc87222f1fb6f221e7501558e5b08_4']\n",
      "['00bc0b7df6951edbe08ac69cfb00f0b7a9927a31_0', '00bc0b7df6951edbe08ac69cfb00f0b7a9927a31_1', '00bc0b7df6951edbe08ac69cfb00f0b7a9927a31_2', '00bde3f3037497aaa1e665ba1b23fc149ec9081f_0', '00c5d8c5b83e61cfcf1bd0dffb0911230d25ce7b_0']\n",
      "['00ce1cf572bcf97c138129087291ba7cce7ed8b3_0', '00ce1cf572bcf97c138129087291ba7cce7ed8b3_1', '00ce1cf572bcf97c138129087291ba7cce7ed8b3_2', '00ce1cf572bcf97c138129087291ba7cce7ed8b3_3', '0339520eef09650d55879247cba49c752a2c0b1e_0']\n",
      "['04085231fc015457761e587ee53e4d1be6eb3666_0', '04085231fc015457761e587ee53e4d1be6eb3666_1', '04085231fc015457761e587ee53e4d1be6eb3666_2', '04085231fc015457761e587ee53e4d1be6eb3666_3', '11708847396_9383898125_b_0']\n",
      "['052e744bb87e87adebaf05d4da1fea43c91519be_0', '052e744bb87e87adebaf05d4da1fea43c91519be_1', '052e744bb87e87adebaf05d4da1fea43c91519be_2', '052e744bb87e87adebaf05d4da1fea43c91519be_3', '060cbff33fb752f1e66ab4154e36b18e98261454_0']\n",
      "['059a1dc3bea8729ee6c43ba3b8280463cf052a6a_0', '059a1dc3bea8729ee6c43ba3b8280463cf052a6a_1', '059a1dc3bea8729ee6c43ba3b8280463cf052a6a_2', '10173736_10152323620229841_233701820_n_0', '10173736_10152323620229841_233701820_n_1']\n"
     ]
    }
   ],
   "source": [
    "training_dataset = image_datasets['train']\n",
    "validation_dataset = image_datasets['val']\n",
    "#test_dataset = image_datasets['test']\n",
    "\n",
    "pri_train = sorted(os.listdir(datadir + 'train/private/'))\n",
    "pub_train = sorted(os.listdir(datadir + 'train/public/'))\n",
    "semi_train = sorted(os.listdir(datadir + 'train/semiprivate/'))\n",
    "\n",
    "pri_val = sorted(os.listdir(datadir + 'val/private/'))\n",
    "pub_val = sorted(os.listdir(datadir + 'val/public/'))\n",
    "semi_val = sorted(os.listdir(datadir + 'val/semiprivate/'))\n",
    "\n",
    "#pri_test = sorted(os.listdir(datadir + 'test/private/'))\n",
    "#pub_test = sorted(os.listdir(datadir + 'test/public/'))\n",
    "#semi_test = sorted(os.listdir(datadir + 'test/semiprivate/'))\n",
    "pri_train_filelist = [x.split('.')[0] for x in pri_train]\n",
    "pub_train_filelist = [x.split('.')[0] for x in pub_train]\n",
    "semi_train_filelist = [x.split('.')[0] for x in semi_train]\n",
    "\n",
    "pri_val_filelist = [x.split('.')[0] for x in pri_val]\n",
    "pub_val_filelist = [x.split('.')[0] for x in pub_val]\n",
    "semi_val_filelist = [x.split('.')[0] for x in semi_val]\n",
    "\n",
    "#pri_test_filelist = [x.split('.')[0] for x in pri_test]\n",
    "#pub_test_filelist = [x.split('.')[0] for x in pub_test]\n",
    "#semi_test_filelist = [x.split('.')[0] for x in semi_test]\n",
    "print(pri_train_filelist[:5])\n",
    "print(pub_train_filelist[:5])\n",
    "print(semi_train_filelist[:5])\n",
    "\n",
    "print(pri_val_filelist[:5])\n",
    "print(pub_val_filelist[:5])\n",
    "print(semi_val_filelist[:5])\n",
    "\n",
    "#print(pri_test_filelist[:5])\n",
    "#print(pub_test_filelist[:5])\n",
    "#print(semi_test_filelist[:5])\n",
    "train_filelist = pri_train_filelist  + pub_train_filelist + semi_train_filelist\n",
    "val_filelist = pri_val_filelist  + pub_val_filelist + semi_val_filelist\n",
    "#test_filelist = pri_test_filelist  + pub_test_filelist + semi_test_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_dir = './Dataset/FaceCoordinates/'\n",
    "coor_traindir = coor_dir + 'train/'\n",
    "coor_validdir = coor_dir + 'val/'\n",
    "coor_testdir = coor_dir + 'test/'\n",
    "\n",
    "fea_dir = './Dataset/FaceFeature/'\n",
    "fea_traindir = fea_dir + 'train/'\n",
    "fea_validdir = fea_dir + 'val/'\n",
    "fea_testdir = fea_dir + 'test/'\n",
    "\n",
    "face_dir = './Dataset/MTCNNAlignedCroppedFaces/'\n",
    "face_traindir = face_dir + 'train/'\n",
    "face_validdir = face_dir + 'val/'\n",
    "face_testdir = face_dir + 'test/'\n",
    "\n",
    "pose_path = './Dataset/inputvector/headpose/'\n",
    "gaze_path = './Dataset/inputvector/eyegaze/'\n",
    "blur_path = './Dataset/inputvector/blur/'\n",
    "position_path = './Dataset/inputvector/faceposition/'\n",
    "\n",
    "input_path = './Dataset/inputvector/input/'\n",
    "label_path = './Dataset/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=6, out_features=10, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dout): Dropout(p=0.2)\n",
      "  (out): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (out_act): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(6, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dout = nn.Dropout(0.2)\n",
    "        #self.fc2 = nn.Linear(10, 20)\n",
    "        #self.prelu = nn.PReLU(1)\n",
    "        self.out = nn.Linear(10, 1)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        a1 = self.fc1(input_)\n",
    "        h1 = self.relu1(a1)\n",
    "        dout = self.dout(h1)\n",
    "        #a2 = self.fc2(dout)\n",
    "        #h2 = self.prelu(a2)\n",
    "        a3 = self.out(dout)\n",
    "        y = self.out_act(a3)\n",
    "        return y\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convert tensor (128, 1, 28, 28) --> (128, 1*28*28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x    \n",
    "\n",
    "net = Net()\n",
    "net.double()\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCELoss(size_average=True)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 6500\n",
      "1000 / 6500\n",
      "2000 / 6500\n",
      "3000 / 6500\n",
      "4000 / 6500\n",
      "5000 / 6500\n",
      "6000 / 6500\n",
      "epoch : 1, train loss : 0.4594, valid loss : 0.4291, valid acc : 110.71%\n",
      "0 / 6500\n",
      "1000 / 6500\n",
      "2000 / 6500\n",
      "3000 / 6500\n",
      "4000 / 6500\n",
      "5000 / 6500\n",
      "6000 / 6500\n",
      "epoch : 2, train loss : 0.4604, valid loss : 0.4291, valid acc : 110.97%\n",
      "0 / 6500\n",
      "1000 / 6500\n",
      "2000 / 6500\n",
      "3000 / 6500\n",
      "4000 / 6500\n",
      "5000 / 6500\n",
      "6000 / 6500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-807683eecf8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_filelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'val/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_filelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mface_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_filelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mimage_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_filelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 447\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for i, inputs in enumerate(train_filelist):\n",
    "        inputs = torch.from_numpy(np.load(input_path + 'train/' + train_filelist[i] + '.npy'))\n",
    "        face_num = int(train_filelist[i].rsplit('_')[-1])\n",
    "        image_name = train_filelist[i].rsplit('_',1)[-2]\n",
    "        with open(label_path + 'train/' + image_name + '.txt') as f:\n",
    "            for j, line in enumerate(f):\n",
    "                if j == face_num:\n",
    "                    target = torch.tensor(np.double(line)).double()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if i % (1000) == 0:\n",
    "            print(f'{i} / 6500')\n",
    "            \n",
    "\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(val_filelist):\n",
    "            inputs = torch.from_numpy(np.load(input_path + 'val/' + val_filelist[i] + '.npy'))\n",
    "            face_num = int(val_filelist[i].rsplit('_')[-1])\n",
    "            image_name = val_filelist[i].rsplit('_',1)[-2]\n",
    "            with open(label_path + 'val/' + image_name + '.txt') as f:\n",
    "                for j, line in enumerate(f):\n",
    "                    if j == face_num:\n",
    "                        target = torch.tensor(np.double(line)).double()\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, target)\n",
    "                    valid_losses.append(loss.item())\n",
    "            \n",
    "                    predicted = outputs\n",
    "                    correct += (predicted == target).sum().item()\n",
    "                    total += target\n",
    "            \n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    \n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, valid acc : {:.2f}%'\\\n",
    "          .format(epoch+1, np.mean(train_losses), np.mean(valid_losses), accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
